---
title: "s13_G6"
author: "Rojas Pachas"
format: html
editor: visual
---

## Grupo 6:

-   Rojas Pachas Lucia

-   Ruíz Ccasa Gean Manuel

-   Huaman Vasquez Takeshi

-   Adrian Llange Rocio

-   Rojas Jesusi Kristy

    **Instalar y cargar los paquetes**

    ```{r}
    install.packages("cluster")
    install.packages("factoextra")
    install.packages("dplyr")
    install.packages("tibble")
    ```

```{r}
library(factoextra)
library(cluster)
library(here)
library(rio)
library(tidyverse)
library(ggplot2)
```

# 1. Análisis de agrupamiento herarquico (Hierarchical Clustering)

## 1. 1. Preparación de los datos

```{r}
datos_cancer <- import (here("data","conoc_actit_factor_cancer_cervical.csv"))
```

### **1.1.1 Solo datos numéricos**

Para aplicar el análisis de agrupamiento jerárquico, seleccionaremos solo las variables **numéricas continuas** del dataset, excluyendo las variables categóricas como `conocimiento`, `n_educacion`, `met_anticoncep`, `procedencia`, `antec_ets`, y `actitud`. Supondremos que `edad` y `edad de inicio de vida sexual` y `numero de parejas sexuales` las variables mas resaltantes , así que este paso lo hacemos para mostrar la estructura

```{r}
library(dplyr)
library(tibble)

datos_cluster <- datos_cancer |>
  mutate(id = row_number()) |>          
  select(id, edad, edad_relacion_sexual, parejas_sex) |> 
  column_to_rownames("id")
```

### 1.1.2 Estandarización de variables

Dado que las variables pueden tener diferentes escalas, es esencial estandarizarlas para que todas contribuyan por igual al análisis de distancias.

```{r}
datos_cluster_escalado <- scale(datos_cluster)
```

Un vistazo a los datos antes y despues del escalamiento:

```{r}
head(datos_cluster)
head(datos_cluster_escalado)
```

## 1.2 Cálculo de distancias

Calculamos las distancias euclidianas entre observaciones.

```{r}
distancia_datos <- dist(datos_cluster_escalado, method = "euclidean")
```

### 1.2.1 Visualización con mapa de calor

```{r}
fviz_dist(distancia_datos)
```

## 1.3 Método de agrupamiento: función de enlace

Usamos el método de **Ward**, adecuado para minimizar la varianza intra-grupo.

```{r}
agrupamiento_jerarquico <- hclust(d = distancia_datos, method = "ward.D2")
```

## 1.4 Visualización del dendograma

```{r}
fviz_dend(agrupamiento_jerarquico, cex = 0.7)
```

## 1.5 Cuantos grupos se formaron?

Para visualizar y resaltar 4 clústeres:

```{r}
fviz_dend(agrupamiento_jerarquico, 
          k = 4, 
          cex = 0.5, 
          k_colors = c("#546500", "#bd2f2c", "#ffbf00","#2E9FDF"),
          color_labels_by_k = TRUE, 
          rect = TRUE)
```

# 2. Agrupamiento con el algoritmo K-Means

El método de agrupamiento K-means es una técnica de aprendizaje no supervisado ampliamente utilizada para clasificar observaciones (en este caso, participantes del estudio sobre cáncer cervical) en grupos homogéneos. A diferencia del agrupamiento jerárquico, aquí el número de grupos (k) debe definirse previamente, lo que permite evaluar directamente cuán bien se ajustan los datos a una estructura grupal dada.

## 2.1Problema y dataset

En este ejercicio continuaremos usando la misma base de datos `datos_cancer`, que fue previamente filtrada para conservar solo variables numéricas, como `edad`, y se estandarizó para el análisis.

```{r}
datos_cancer <- datos_cancer |>
  mutate(id = row_number())

```

```{r}
library(dplyr)
library(tibble)

# Selección y preparación de datos
datos_cluster <- datos_cancer |>
  select(id, edad, edad_relacion_sexual, parejas_sex) |> 
  column_to_rownames("id") |>
  scale()

```

```{r}
fviz_nbclust(
  datos_cluster,        
  kmeans,
  nstart = 25,
  method = "wss"        
) +
  geom_vline(xintercept = 4, linetype = 2) +
  labs(
    title = "Número óptimo de clústeres - Método del codo",
    x = "Número de clústeres",
    y = "Suma de cuadrados intra-clúster (WSS)"
  )
```

**Interpretación:** El gráfico generado por la función `fviz_nbclust()` representa la suma de cuadrados dentro de los clústeres (WSS) en función del número de clústeres (k) considerados. A medida que aumentamos el número de clústeres, la WSS disminuye, ya que los grupos son más pequeños y homogéneos. Sin embargo, hay un punto donde el beneficio de agregar más clústeres se vuelve marginal; este punto se conoce como "el codo" de la curva.

En nuestro análisis, el gráfico sugiere que el "codo" se encuentra alrededor de **k = 4**, lo que indica que dividir a las participantes en **4 grupos** es una elección adecuada, ya que balancea simplicidad y capacidad de capturar patrones relevantes en los datos.

Esto implica que, con base en las variables **edad actual**, **edad de inicio de vida sexual** y **número de parejas sexuales**, se pueden identificar **cuatro perfiles diferenciados** de participantes dentro del conjunto de datos. Este agrupamiento puede resultar útil para posteriores análisis comparativos entre grupos, como evaluar diferencias en el nivel de conocimiento sobre el cáncer cervical, actitudes hacia la prevención, o el uso de métodos anticonceptivos.

### 2.2 Cálculo del agrupamiento k-means

Realizamos el agrupamiento especificando `k = 4` (como ejemplo) y usando múltiples inicializaciones (`nstart = 25`) para mejorar la estabilidad del resultado.

```{r}
set.seed(123)
km_res <- kmeans(datos_cluster, centers = 4, nstart = 25)
```

```{r}
km_res
```

Este resultado incluye:

-   **Cluster means:** Promedio de las variables por cada clúster.

-   **Clustering vector:** A qué clúster pertenece cada individuo.

## 2.3 Visualización de los clústeres

```{r}
fviz_cluster(
  km_res,
  data = datos_cluster ,
  palette = c("#2E9FDF", "#E7B800", "#FC4E07", "#1B9E77" ),
  ellipse.type = "euclid",
  repel = TRUE,
  ggtheme = theme_minimal()
)
```

**Interpretación del gráfico de agrupamiento K-means**

En el gráfico generado por el algoritmo K-means, cada punto representa a una participante del estudio y ha sido clasificado en uno de los **cuatro grupos (clústeres)** según tres variables: **edad actual**, **edad de inicio de vida sexual** y **número de parejas sexuales**. Estas variables fueron previamente **estandarizadas** para asegurar que todas tuvieran el mismo peso en el análisis, y luego transformadas mediante **Análisis de Componentes Principales (PCA)** para permitir una visualización bidimensional que conserva la mayor parte de la información de los datos originales.

El algoritmo identificó **cuatro clústeres diferenciables**:

-   **Clúster 1 (color celeste)**: Agrupa a participantes con mayor edad, inicio de vida sexual más tardío y mayor número de parejas sexuales. Representa un perfil de mujeres con mayor experiencia y posiblemente más exposición acumulada.

-   **Clúster 2 (amarillo)**: Incluye a mujeres con edad media y un inicio de vida sexual más temprano. El número de parejas sexuales tiende a ser intermedio. Este grupo podría reflejar una transición entre los extremos del conjunto de datos.

-   **Clúster 3 (rojo)**: Representa a participantes más jóvenes, con inicio de relaciones sexuales también temprano y número bajo de parejas. Podría tratarse de mujeres adolescentes o jóvenes adultas con menor historial sexual.

-   **Clúster 4 (verde)**: Es el grupo más amplio y disperso, que contiene perfiles intermedios o heterogéneos. La variabilidad en este clúster sugiere que no hay un único patrón claro, lo cual es común en análisis poblacionales.

La **distribución y separación de los grupos** en el gráfico indican la presencia de **patrones latentes** que segmentan a las participantes en perfiles distintos. La forma y proximidad entre clústeres también puede sugerir relaciones entre las variables y la posible transición de un perfil a otro.

Sin embargo, como todo análisis exploratorio, **este resultado no implica causalidad**, y debe ser interpretado con cautela

```{r}

dendro_plot <- fviz_dend(hc, k = 4, rect = TRUE)
ggsave("dendrograma.png", plot = dendro_plot, width = 8, height = 6, dpi = 300)

```

```{r}
ggsave("dendrograma.col.png", plot = dendro_plot, width = 8, height = 6, dpi = 300)
```

```{r}
codo_plot <- fviz_nbclust(
  datos_cluster,        
  kmeans,
  nstart = 25,
  method = "wss"
) +
  geom_vline(xintercept = 4, linetype = 2) +
  labs(
    title = "Número óptimo de clústeres - Método del codo",
    x = "Número de clústeres",
    y = "Suma de cuadrados intra-clúster (WSS)"
  )
ggsave("grafico_codo.png", plot = codo_plot, width = 8, height = 6, dpi = 300)
```

```{r}
cluster_plot <- fviz_cluster(
  km_res,
  data = datos_cluster,
  palette = c("#2E9FDF", "#E7B800", "#FC4E07", "#1B9E77"),
  ellipse.type = "euclid",
  repel = TRUE,
  ggtheme = theme_minimal()
)

# 2. Guardar el gráfico como PNG
ggsave("grafico_cluster.png", plot = cluster_plot, width = 8, height = 6, dpi = 300)
```

```{r}
# 1. Crear el gráfico y guardarlo en un objeto
dist_plot <- fviz_dist(distancia_datos)

# 2. Guardar el gráfico como imagen PNG
ggsave("grafico_distancias.png", plot = dist_plot, width = 8, height = 6, dpi = 300)
```
